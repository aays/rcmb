"""
summarise_cross.py - summarise putative calls into a table
"""

import os
import sys
import csv
import subprocess
import numpy as np
from readcomb.classification import Pair
import argparse
import pysam
from tqdm import tqdm
from multiprocessing import Queue
from multiprocessing import Process

def arg_parser():
    parser = argparse.ArgumentParser(
        description='summarise putative recombinant calls', 
        usage='python summarise_cross.py [options]')

    parser.add_argument('-f', '--bam', required=True,
        type=str, help='BAM output from readcomb-filter and readcomb-fp')
    parser.add_argument('-v', '--vcf', required=True,
        type=str, help='VCF with calls for cross of interest')
    parser.add_argument('--false_bed', required=False,
        type=str, help='BED file generated by readcomb-fp with nuclear setting')
    parser.add_argument('--microsats', required=False, type=str, 
        nargs='+', help='Path to parent1 microsat file and parent2 microsat file, in that order')
    parser.add_argument('-m', '--mask_size', required=True,
        type=int, help='Masking size')
    parser.add_argument('-p', '--processes', required=False, default=1,
        type=int, help='Number of processes to use (default 1)')
    parser.add_argument('-q', '--mapq', required=False,
        default=50, type=int, help='MAPQ filter')
    parser.add_argument('--base_qual', required=False,
        default=50, type=int, help='base qual filter')
    parser.add_argument('--remove_uninformative', required=False,
        action='store_true', help='Ignore all no phase change reads')
    parser.add_argument('-o', '--out', required=True,
        type=str, help='File to write to')

    return parser


class Processor(Process):
    """
    inherited from multiprocessing.Process

    writes desired pair info to a csv file - each process gets its own temp csv
    file before the files are concatenated

    processes are given pairs of reads to then assemble `rc.Pair` objects from
    and then classify + output quality metrics
    """
    def __init__(self, input_queue, counter_queue, args, p_id, **kwargs):
        Process.__init__(self, **kwargs)
        self.input_queue = input_queue
        self.counter_queue = counter_queue
        self.args = args
        self.p_id = p_id
        self.header = pysam.AlignmentFile(args.bam, 'r').header
        self.fieldnames = [
            'chromosome', 'midpoint', 'start', 'end', 'rel_midpoint', 'call', 'masked_call',
            'mask_size', 'var_count', 'outer_bound', 'min_end_proximity', 'min_vars_in_hap', 
            'var_skew', 'mismatch_var_ratio', 'var_per_hap', 'gc_length', 'indel_count',
            'indel_proximity', 'proximate_indel_length', 'false_overlap',
            'parent1_microsat', 'parent2_microsat', 'parent1_microsat_length',
            'parent2_microsat_length', 'parent1_microsat_log2_pval',
            'parent2_microsat_log2_pval', 'parent1_microsat_score',
            'parent2_microsat_score', 'parent1_microsat_proximity',
            'parent2_microsat_proximity', 'read1_mapq', 'read2_mapq', 
            'min_var_qual', 'min_var_depth', 'avg_diff_parental_gq', 'avg_hap_var_gq', 
            'avg_hap_var_depth', 'min_base_qual', 'mean_base_qual', 'min_phase_change_var_qual', 
            'read1_length', 'read2_length', 'effective_length', 'detection', 'read_name']

    def run(self):

        with open(self.args.out + f'{self.p_id}', 'w', newline='') as f:
            counter = 0
            writer = csv.DictWriter(f, delimiter='\t', fieldnames=self.fieldnames)
            writer.writeheader()
            current_pair = self.input_queue.get(block=True)
            counter += 1

            # false bed from nuclear readcomb-fp provided for false_overlap metric
            if self.args.false_bed:
                bed_reader = pysam.TabixFile(self.args.false_bed)

            if self.args.microsats:
                parent1_microsat_reader = pysam.TabixFile(self.args.microsats[0])
                parent2_microsat_reader = pysam.TabixFile(self.args.microsats[1])
            else:
                raise ValueError('microsat files not provided')

            while current_pair:
                counter += 1
                self.counter_queue.put(1) # increment counter
                record_1 = pysam.AlignedSegment.fromstring(current_pair[0], self.header)
                record_2 = pysam.AlignedSegment.fromstring(current_pair[1], self.header)
                if not current_pair:
                    break

                pair = Pair(record_1, record_2, self.args.vcf)

                if all([pair.rec_1.mapq >= self.args.mapq, pair.rec_2.mapq >= self.args.mapq]):
                    # hardcoding path
                    mvr_vcf = self.args.vcf.replace('vcf_filtered', 'vcf_freebayes')
                    pair.classify(
                        masking=self.args.mask_size, 
                        quality=self.args.base_qual,
                        mvr_vcf_filepath=mvr_vcf)
                else:
                    current_pair = self.input_queue.get(block=True)
                    continue

                if self.args.remove_uninformative and pair.call == 'no_phase_change':
                    current_pair = self.input_queue.get(block=True)
                    continue
                else:
                    start = pair.rec_1.reference_start
                    end = pair.rec_2.reference_start + len(pair.segment_2)
                    if not 'N' in [hap for hap, _, _, _ in pair.detection]:
                        pair_vars = pair.variants_filt
                        detection_indices = [int(hap) - 1 for hap, _, _, _ in pair.detection]
                    else:
                        # remove no match variant from detection and variants_filt
                        no_match_idx = [i for i, var in enumerate(pair.detection) if var[0] == 'N']
                        detection_indices = [
                            int(hap) - 1 for hap in 
                            [hap for hap, _, _, _ in pair.detection if hap != 'N']]
                        pair_vars = [
                            v for i, v in enumerate(pair.variants_filt)
                            if i not in no_match_idx]

                    out_dict = {
                        'chromosome': pair.rec_1.reference_name, 
                        'midpoint': pair.midpoint,
                        'start': pair.rec_1.reference_start,
                        'end': pair.rec_2.reference_end,
                        'rel_midpoint': pair.relative_midpoint,
                        'call': pair.call, 
                        'masked_call': pair.masked_call,
                        'mask_size': self.args.mask_size, 
                        'var_count': len(pair.variants_filt),
                        'outer_bound': pair.outer_bound,
                        'min_end_proximity': pair.min_end_proximity,
                        'min_vars_in_hap': pair.min_variants_in_haplotype,
                        'var_skew': pair.variant_skew, 
                        'mismatch_var_ratio': round(pair.mismatch_variant_ratio, 2),
                        'var_per_hap': pair.variants_per_haplotype,
                        'gc_length': pair.gene_conversion_len,
                        'indel_count': len(pair.indels) if pair.indels else 0,
                        'indel_proximity': pair.indel_proximity,
                        'proximate_indel_length': pair.proximate_indel_length,
                        'read1_mapq': pair.rec_1.mapq, 'read2_mapq': pair.rec_2.mapq,
                        'min_base_qual': pair.min_base_qual,
                        'mean_base_qual': pair.mean_base_qual,
                        'read1_length': len(pair.rec_1.query_sequence),
                        'read2_length': len(pair.rec_2.query_sequence),
                        'effective_length': end - start,
                        'detection': pair.detection,
                        'read_name': pair.rec_1.query_name
                        }

                    if self.args.false_bed:
                        # check whether a nuclear read overlaps
                        try:
                            false_lookup = list(set([
                                line for line in
                                bed_reader.fetch(
                                    pair.rec_1.reference_name,
                                    start, end)]))
                            out_dict['false_overlap'] = 1 if len(false_lookup) > 0 else 0
                        except ValueError: # iterator cannot be generated for region
                            # usually cause mtDNA etc
                            out_dict['false_overlap'] = 0
                    else:
                        out_dict['false_overlap'] = 'NA'

                    def process_microsats(micro_bed_reader, chrom, start, end, midpoint):
                        try:
                            microsat_lookup = [line for line in
                                micro_bed_reader.fetch(chrom, start, end)]
                        except ValueError:
                            if chrom in ['mtDNA', 'cpDNA']:
                                return 0, 0, 'NA', 'NA', 'NA'
                            else:
                                print(pair)
                                raise Exception('microsat lookup went wrong')

                        if microsat_lookup:
                            if len(microsat_lookup) > 1:
                                # keep the highest scoring one
                                scores = [float(line.split('\t')[5]) for line in microsat_lookup]
                                highest_score = max(scores)
                                microsat_to_keep = [line for line in microsat_lookup
                                    if float(line.split('\t')[5]) == highest_score]
                                sp = microsat_to_keep[0].split('\t')
                            else:
                                sp = microsat_lookup[0].split('\t')
                            microsat_length = sp[3]
                            microsat_pval = sp[6]
                            microsat_score = sp[5]
                            pair_midpoint = int(midpoint.split(':')[1])
                            microsat_prox = min([
                                abs(pair_midpoint - int(sp[1])),
                                abs(pair_midpoint - int(sp[2]))])
                            return 1, microsat_length, microsat_pval, microsat_score, microsat_prox
                            
                        elif not microsat_lookup:
                            return 0, 0, 'NA', 'NA', 'NA' # microsat, length, pval, score, prox

                    # check whether microsat overlaps in either parent
                    out_dict['parent1_microsat'], \
                    out_dict['parent1_microsat_length'], \
                    out_dict['parent1_microsat_log2_pval'], \
                    out_dict['parent1_microsat_score'], \
                    out_dict['parent1_microsat_proximity'] = process_microsats(
                        parent1_microsat_reader, 
                        pair.rec_1.reference_name,
                        start, end, pair.midpoint)
                    out_dict['parent2_microsat'], \
                    out_dict['parent2_microsat_length'], \
                    out_dict['parent2_microsat_log2_pval'], \
                    out_dict['parent2_microsat_score'], \
                    out_dict['parent2_microsat_proximity'] = process_microsats(
                        parent2_microsat_reader, 
                        pair.rec_1.reference_name,
                        start, end, pair.midpoint)
                            
                    if pair_vars:
                        out_dict['min_phase_change_var_qual'] = min(
                            qual for _, pos, _, qual in pair.detection
                             if pos in pair.phase_change_variants) if not \
                             pair.call == 'no_phase_change' else -1
                        out_dict['min_var_qual'] = round(min([v.QUAL for v in pair_vars]), 2)
                        out_dict['min_var_depth'] = min(np.concatenate([v.gt_depths for v in pair_vars]))
                        out_dict['avg_diff_parental_gq'] = round(
                            sum([abs(v.gt_quals[0] - v.gt_quals[1]) 
                            for v in pair_vars]) / len(pair_vars), 2)
                        out_dict['avg_hap_var_gq'] = round(
                            sum(v.gt_quals[detection_indices[i]]
                            for i, v in enumerate(pair_vars)) / len(pair_vars), 2)
                        out_dict['avg_hap_var_depth'] = round(
                            sum(v.gt_depths[detection_indices[i]]
                            for i, v in enumerate(pair_vars)) / len(pair_vars), 2)
                    else:
                        out_dict['min_phase_change_var_qual'] = -1
                        out_dict['min_var_qual'] = -1
                        out_dict['min_var_depth'] = -1
                        out_dict['avg_diff_parental_gq'] = -1
                        out_dict['avg_hap_var_gq'] = -1
                        out_dict['avg_hap_var_depth'] = -1
                    writer.writerow(out_dict)

                current_pair = self.input_queue.get(block=True)

class Counter(Process):
    """
    counter object for logging if implementation is ever needed

    currently just creates a tqdm progress bar, could be extended to some
    logging functionality but probably not all that needed
    """
    def __init__(self, input_queue, args, **kwargs):
        Process.__init__(self, **kwargs)
        self.input_queue = input_queue
        self.args = args
    
    def run(self):
        ps = subprocess.Popen(['grep', '-v', '@', self.args.bam], stdout=subprocess.PIPE)
        output = subprocess.run(['wc', '-l'], stdin=ps.stdout, stdout=subprocess.PIPE)
        line_count = int(int(output.stdout.decode('utf-8').rstrip('\n')) / 2)
        self.progress = tqdm(total=line_count)

        count = self.input_queue.get(block=True)

        while count:
            self.progress.update(n=1)
            count = self.input_queue.get(block=True)

        self.progress.close()


def parse_reads_parallel(args):
    """parse reads w/ multiprocessing if needed and write to file

    Parameters
    ----------
    args : argparse.ArgumentParser
        command line args

    Returns
    -------
    None
    """

    print('[readcomb] creating processes')
    processes = []
    input_queues = []

    count_input = Queue()
    counter = Counter(count_input, args, daemon=True)
    counter.start()

    for i in range(args.processes):
        input_queue = Queue()
        input_queues.append(input_queue)
        processes.append(
            Processor(input_queue=input_queue, 
                counter_queue=count_input,
                args=args, p_id=i, daemon=True))
        processes[i].start()

    print('[readcomb] processes created')

    bam = pysam.AlignmentFile(args.bam, 'r')
    prev_record = None
    process_idx = 0

    pair_count = 0
    for record in tqdm(bam):
        if not prev_record:
            prev_record = record
        else:
            if prev_record.query_name != record.query_name:
                raise ValueError(f'read {record.query_name} not paired')

            pair = [prev_record.to_string(), record.to_string()]

            input_queues[process_idx].put(pair)
            pair_count += 1

            if process_idx < args.processes - 1:
                process_idx += 1
            else:
                process_idx = 0
            prev_record = None

    for i, _ in enumerate(processes):
        input_queues[i].put(None)
        input_queues[i].close()
        processes[i].join()

    count_input.put(None)
    count_input.close()
    counter.join()

    print('[readcomb] concatenating temp files')
    with open(args.out, 'w', newline='') as f_out:
        first = True
        for temp_file in [f'{args.out}{i}' for i, _ in enumerate(processes)]:
            with open(temp_file, 'r', newline='') as f_in:
                reader = csv.DictReader(f_in, delimiter='\t')
                if first:
                    writer = csv.DictWriter(f_out, delimiter='\t',
                        fieldnames=reader.fieldnames)
                    writer.writeheader()
                    first = False
                for line in reader:
                    writer.writerow(line)
            os.remove(temp_file)
                    

def main():
    parser = arg_parser()
    parse_reads_parallel(parser.parse_args())

if __name__ == '__main__':
    main()

        

