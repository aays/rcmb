
## 3/6/2022

today - installing LDhelmet 1.10 on the server and testing it using
the provided example file

```bash
cd ~/apps
wget https://github.com/popgenmethods/LDhelmet/archive/refs/heads/main.zip
# modified Makefile to add libraries 
# /research/tmp_apps/my_boost/include and /research/tmp_apps/my_gsl/include
make
```

running the example workflow - to do once phase changes and such are done,
since I'm hammering the server enough as it stands

```bash
cd example_scripts
time bash run_all.bash
```

## 29/7/2022

finally getting back to work on this - need to generate parental fastas
to then feed as input to LDhelmet

seems the best way to do this is to use `bcftools concat` on a GVCF - though
for this I'll have to remake the 3071 GVCF, but that's about it (since the 2x250
one was totally bonked, and I had to revert to 2x150)

let's first try this on a GVCF that I know is good to go:

```bash
time bcftools consensus \
--fasta-ref data/references/CC4532.w_organelles_MTplus.fa \
--output 1691_consensus_test.fasta \
data/genotyping/gvcf_sample/CC1691.g.vcf.gz
```

this looks good to go, and runs super quick (done in 20 sec!) 

I'm going to get the 3071 GVCF going for now - after this, will have to make
FASTAs for each chromosome, with `chromosome_n.fasta` containing said
chromosome from each sample - should be a quick bit of SeqIO wrangling

here goes the 3071 GVCF - this will run overnight

```bash
time gatk HaplotypeCaller \
-R data/references/CC4532.w_organelles_MTplus.fa \
-I data/alignments/parental_bam/CC3071.bam \
-ERC GVCF -ploidy 2 \
--heterozygosity 0.02 --indel-heterozygosity 0.002 \
--native-pair-hmm-threads 2 \
-O data/genotyping/gvcf_sample/CC3071.g.vcf.gz
# took 14.5 hours
```

## 30/7/2022

today - creating fastas for each of the samples,
and then converting those fastas into per-chr
files

let's get our directories in order and then get things going:

```bash
mkdir -p data/ldhelmet/
mkdir -p data/ldhelmet/fasta
mkdir -p data/ldhelmet/fasta_chrom
```

I just realized though that I probably should
use a snakemake workflow for this whole thing,
and that includes generating the fastas

will need an ancillary script for generating the per-chr
files from the per-sample files but otherwise I should be good

```bash
# contains a rule called create_sample_fasta
# rule all currently set to output of that rule
time snakemake -pr -s analysis/ldhelmet/ldhelmet.smk # done in 7 min
```

and now to convert these fastas into per-chr files -
for this, I think I should use snakemake's script feature
and just create a small custom script for this

will need to provide all filenames at once using `expand()`
for this I think, and then update rule all with per-chr files

```bash
# updated rule all to include per-chr fastas in new dir
time snakemake -pr -s analysis/ldhelmet/ldhelmet.smk # took 10 min
```

looks great! tomorrow - set up LDhelmet rules (maybe with separate scripts?
figure this out) for NA1-only, NA2-only, and NA1xNA2 (ie all samples)

## 1/8/2022

getting on this now - going to try to make all these rules at once, since
most ldhelmet files are temporary

working off the `reinhardtii-ld-rcmb` logs here - going to reuse the same
hotspot definition etc since I've already validated that in chlamy with simulations

but first - I messed up the fasta making the other day - I need to make
three separate ones for each chrom, for each of NA1/NA2/all strains

updated the rule - here goes again:

```bash
# should now make three sets of each file in fasta_chrom
# prepending chrom name with one of NA1/NA2/ALL
time snakemake -pr -s analysis/ldhelmet/ldhelmet.smk
```

## 2/8/2022

and now to set up the LDhelmet rules - going to symlink the executable
into `bin` and create three dirs in `data/ldhelmet/ldhelmet` - one for each pop
(ALL, NA1, NA2)

need to also get the mutation matrix file from `reinhardtii-ld-rcmb` - going to
copy that into `data/ldhelmet/`

alright - looks like this is going to run for a while, but the journey of a thousand
lookup tables begins with a single step:

```bash
# being a bit greedy with cores - all rules are permitted no more than 10 except rjmcmc
# implements all steps and designates all intermediate files as temp
time snakemake -pr -s analysis/ldhelmet/ldhelmet.smk --cores 30
```

## 4/8/2022

LDhelmet keeps breaking - I think there might be an issue with the fastas

it seems they're of separate lengths and I wonder if vcf2fasta fixes that

```bash
# testing
# grabbed vcf2fasta from the salt bin folder

time python bin/vcf2fasta.py -v data/genotyping/gvcf_sample/CC1691.g.vcf.gz \
-r data/references/CC4532.w_organelles_MTplus.fa \
-i chromosome_01:1-8225636 -s CC1691 --min_GQ 30 > 1691_test.fa
```

no wonder LDhelmet is breaking - the fastas generated by `bcftools consensus`
have incorrect lengths:

```python
>>> from Bio import SeqIO
>>> reader = SeqIO.parse('data/ldhelmet/fasta_chrom/ALL.chromosome_1.fa', 'fasta')
>>> for record in reader:
...     print(record.id, len(record.seq))
CC2935 8245691
CC1691 8233988
CC3071 8210741
CC3086 8243129
CC3059 8247740
CC2932 8258955
CC2931 8258531
CC1952 8261646
CC2343 8262206
CC3062 8248875
CC2342 8258027
GB119 8245813
CC2344 8259022
```

when the chromosome itself is literally 8225636 bp long - why are some of these
multiple kbp longer? 

in either case let's redo the fasta generation with vcf2fasta instead - though I have
to get a dictionary of chromosome lengths

here goes:

```bash
# will just run create_indiv_fastas rule
# creates sample-chrom pairs in data/ldhelmet/fasta
# will merge into per-chr files separated by pop after
time snakemake -pr -s analysis/ldhelmet/ldhelmet.smk --cores 4
```

## 5/8/2022

took 8 hours! now to merge into per-chr files - updated `convert_fastas.py`
accordingly

```bash
# refactored rule all and such to accommodate chrom names with leading 0s
# will run rule to create per chr fastas
time snakemake -pr -s analysis/ldhelmet/ldhelmet.smk --cores 4
```

## 11/8/2022

so this keeps seg faulting at `ALL.chromosome_12` no matter how many cores I use -
even setting it to one thread resulted in a segfault after 3 days of running

going to disable the 'ALL' files for now and see if that makes things any better








